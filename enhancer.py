#!/usr/bin/env python3
"""
Respector-LLM Enhancer: Hybrid Documentation Generation

This module enhances OpenAPI specifications generated by Respector by adding
semantic documentation (summary and description fields) using Azure OpenAI GPT-4o.

Based on "Generating REST API Specifications through Static Analysis" (ICSE '24)
"""

import argparse
import json
import os
import sys
import time
from typing import Any

from dotenv import load_dotenv
from openai import AzureOpenAI
from tqdm import tqdm  # type: ignore

# Load environment variables from .env file
load_dotenv()

# System prompt for the LLM
SYSTEM_PROMPT = """You are an expert Technical Writer for REST APIs. 
Your goal is to generate professional, concise documentation based on technical constraints extracted from static code analysis.

You must respond ONLY with valid JSON. No markdown, no explanation, just the JSON object."""

# User prompt template
USER_PROMPT_TEMPLATE = """Analyze the following API Endpoint metadata:
1. Operation Name (Java Method): {operation_id}
2. HTTP Method: {method}
3. URL Path: {path}
4. Parameters: {parameters}
5. Request Body Schema: {request_body}
6. Response Codes: {response_codes}
7. Identified Constraints: {constraints}

Task:
- Generate a 'summary' (max 50 chars) that describes the endpoint purpose.
- Generate a 'description' that explains the endpoint's purpose and mentions valid input ranges based on the constraints provided.
- Return strict JSON format with exactly these keys: "summary", "description"

Example output:
{{"summary": "Get Country by Alpha Code", "description": "Retrieves country information by ISO alpha-2 or alpha-3 code. The code must be between 2-3 characters."}}"""


def create_azure_client() -> AzureOpenAI:
    """Create and return an Azure OpenAI client."""
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
    
    if not api_key or not endpoint:
        raise ValueError(
            "Missing Azure OpenAI credentials. "
            "Please set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT environment variables."
        )
    
    return AzureOpenAI(
        api_key=api_key,
        api_version=api_version,
        azure_endpoint=endpoint
    )


def extract_constraints(spec: dict, path: str, method: str) -> dict:
    """Extract constraints from the x-endpoint-constraints component."""
    constraints = {}
    
    # Navigate to components/x-endpoint-constraints
    components = spec.get("components", {})
    endpoint_constraints = components.get("x-endpoint-constraints", {})
    
    # Find constraints for this path/method
    path_constraints = endpoint_constraints.get(path, {})
    method_constraints = path_constraints.get(method, {})
    
    if method_constraints:
        # Extract parameter constraints
        param_constraints = method_constraints.get("x-parameter-constraints", {})
        if param_constraints:
            constraints["parameter_constraints"] = param_constraints
        
        # Extract global reads (for context)
        global_reads = method_constraints.get("global-reads", {})
        if global_reads:
            constraints["global_reads"] = [
                v.get("name", "") for v in global_reads.values()
            ]
    
    return constraints


def extract_parameters_info(parameters: list) -> str:
    """Extract readable parameter information."""
    if not parameters:
        return "None"
    
    param_info = []
    for param in parameters:
        name = param.get("name", "unknown")
        location = param.get("in", "unknown")
        required = param.get("required", False)
        schema = param.get("schema", {})
        
        info = f"{name} ({location}, {'required' if required else 'optional'})"
        
        # Add schema constraints
        constraints = []
        if "minLength" in schema:
            constraints.append(f"minLength: {schema['minLength']}")
        if "maxLength" in schema:
            constraints.append(f"maxLength: {schema['maxLength']}")
        if "minimum" in schema:
            constraints.append(f"min: {schema['minimum']}")
        if "maximum" in schema:
            constraints.append(f"max: {schema['maximum']}")
        if "type" in schema:
            constraints.append(f"type: {schema['type']}")
            
        if constraints:
            info += f" [{', '.join(constraints)}]"
        
        param_info.append(info)
    
    return "; ".join(param_info)


def extract_request_body_info(operation: dict) -> str:
    """Extract readable request body information."""
    request_body = operation.get("requestBody", {})
    if not request_body:
        return "None"
    
    content = request_body.get("content", {})
    json_content = content.get("application/json", {})
    schema = json_content.get("schema", {})
    
    if not schema:
        return "JSON body required"
    
    properties = schema.get("properties", {})
    if properties:
        props = []
        for name, prop_schema in properties.items():
            prop_type = prop_schema.get("type", "unknown")
            props.append(f"{name}: {prop_type}")
        return f"JSON with properties: {', '.join(props)}"
    
    return "JSON body required"


def extract_response_codes(operation: dict) -> str:
    """Extract response codes and their descriptions."""
    responses = operation.get("responses", {})
    codes = []
    for code, resp in responses.items():
        if code != "default":
            desc = resp.get("description", "")
            codes.append(f"{code}: {desc}")
    return "; ".join(codes) if codes else "No specific response codes"


def generate_documentation(
    client: AzureOpenAI,
    deployment_name: str,
    operation_id: str,
    method: str,
    path: str,
    parameters: str,
    request_body: str,
    response_codes: str,
    constraints: dict
) -> dict:
    """Generate documentation using Azure OpenAI."""
    
    prompt = USER_PROMPT_TEMPLATE.format(
        operation_id=operation_id,
        method=method.upper(),
        path=path,
        parameters=parameters,
        request_body=request_body,
        response_codes=response_codes,
        constraints=json.dumps(constraints, indent=2) if constraints else "None identified"
    )
    
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content
        if content is None:
            raise ValueError("Empty response from LLM")
        return json.loads(content)
        
    except json.JSONDecodeError as e:
        print(f"  Warning: Failed to parse LLM response as JSON: {e}", file=sys.stderr)
        return {
            "summary": f"{method.upper()} {path}",
            "description": f"Endpoint for {operation_id}"
        }
    except Exception as e:
        print(f"  Warning: LLM API error: {e}", file=sys.stderr)
        return {
            "summary": f"{method.upper()} {path}",
            "description": f"Endpoint for {operation_id}"
        }


def enhance_spec(input_path: str, output_path: str, verbose: bool = False) -> None:
    """
    Main function to enhance an OpenAPI spec with LLM-generated documentation.
    
    Args:
        input_path: Path to the Respector-generated JSON spec
        output_path: Path for the enhanced output spec
        verbose: Whether to print progress information
    """
    # Load the input spec
    print(f"üìÇ Loading spec from: {input_path}")
    
    with open(input_path, "r", encoding="utf-8") as f:
        spec = json.load(f)
    
    # Create Azure OpenAI client
    print("üîå Connecting to Azure OpenAI...")
    client = create_azure_client()
    deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
    print(f"‚úì Using deployment: {deployment_name}")
    
    # Collect all endpoints to process
    paths = spec.get("paths", {})
    endpoints = []
    for path, operations in paths.items():
        for method, operation in operations.items():
            if method in ["get", "post", "put", "delete", "patch"]:
                endpoints.append((path, method, operation))
    
    print(f"üìã Found {len(endpoints)} endpoints to enhance\n")
    
    # Process endpoints with progress bar
    successful = 0
    failed = 0
    
    with tqdm(
        endpoints,
        desc="üöÄ Enhancing endpoints",
        unit="endpoint",
        ncols=100,
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
    ) as pbar:
        for path, method, operation in pbar:
            operation_id = operation.get("operationId", f"{method}_{path}")
            
            # Update progress bar description
            pbar.set_postfix_str(f"{method.upper()} {path[:30]}...")
            
            # Extract metadata
            parameters = extract_parameters_info(operation.get("parameters", []))
            request_body = extract_request_body_info(operation)
            response_codes = extract_response_codes(operation)
            constraints = extract_constraints(spec, path, method)
            
            # Generate documentation
            docs = generate_documentation(
                client=client,
                deployment_name=deployment_name,
                operation_id=operation_id,
                method=method,
                path=path,
                parameters=parameters,
                request_body=request_body,
                response_codes=response_codes,
                constraints=constraints
            )
            
            # Check if we got a valid response
            if docs.get("summary") and docs.get("description"):
                successful += 1
            else:
                failed += 1
            
            # Inject documentation into the spec
            operation["summary"] = docs.get("summary", "")[:50]  # Enforce max 50 chars
            operation["description"] = docs.get("description", "")
            operation["x-enhanced-by"] = "Respector-LLM"
            
            # Small delay to respect rate limits
            time.sleep(0.5)
    
    # Update spec metadata
    if "info" in spec:
        original_desc = spec["info"].get("description", "")
        spec["info"]["description"] = f"{original_desc}\n\nEnhanced with Respector-LLM Enhancer."
    
    # Write enhanced spec
    print(f"\nüíæ Writing enhanced spec to: {output_path}")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(spec, f, indent=2, ensure_ascii=False)
    
    # Print summary
    print(f"\n{'='*50}")
    print(f"‚úÖ Enhancement complete!")
    print(f"{'='*50}")
    print(f"   Total endpoints:  {len(endpoints)}")
    print(f"   Successful:       {successful}")
    if failed > 0:
        print(f"   ‚ö†Ô∏è  Fallback used: {failed}")
    print(f"   Output file:      {output_path}")
    print(f"{'='*50}")


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Respector-LLM Enhancer: Add semantic documentation to OpenAPI specs"
    )
    parser.add_argument(
        "input",
        help="Path to the Respector-generated OpenAPI JSON spec"
    )
    parser.add_argument(
        "-o", "--output",
        help="Output path for enhanced spec (default: respector-enhanced-generated/<filename>)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Print progress information"
    )
    
    args = parser.parse_args()
    
    # Determine output path
    if args.output:
        output_path = args.output
    else:
        # Default: save to respector-enhanced-generated/ directory
        input_dir = os.path.dirname(args.input)
        base_dir = os.path.dirname(input_dir) if input_dir else "."
        output_dir = os.path.join(base_dir, "respector-enhanced-generated")
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.basename(args.input)
        output_path = os.path.join(output_dir, filename)
    
    # Run enhancement
    try:
        enhance_spec(args.input, output_path, args.verbose)
    except FileNotFoundError:
        print(f"Error: Input file not found: {args.input}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()

