# Respector-LLM Enhancer

**Hybrid Documentation Generation for OpenAPI Specifications**

Enhances OpenAPI specifications generated by [Respector](https://github.com/nntzuekai/Respector) by adding semantic documentation (summary and description fields) using Azure OpenAI GPT-4o.

Based on *"Generating REST API Specifications through Static Analysis"* (ICSE '24)

## Overview

While Respector excels at extracting exact technical structure (routes, parameters, constraints) through static and symbolic analysis, the generated specifications (OpenAPI 3.0) lack the semantic information developers need (`summary` and `description` fields).

**Respector-LLM Enhancer** is a post-processing module that uses a Large Language Model (Azure OpenAI GPT-4o) to interpret the technical metadata extracted by Respector and generate natural language documentation.

## Features

- Parses Respector-generated OpenAPI JSON specifications
- Extracts endpoint metadata (operationId, parameters, constraints)
- Generates professional API documentation using Azure OpenAI
- Injects documentation back into the specification
- Marks enhanced endpoints with `x-enhanced-by: Respector-LLM`

## Installation

```bash
# Clone the repository
git clone https://github.com/your-repo/respector-llm.git
cd respector-llm

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

**Quick Start:** See [QUICK-START.md](QUICK-START.md) for a step-by-step guide.

## Configuration

1. Copy the configuration template:
   ```bash
   cp config.example.env .env
   ```

2. Edit `.env` with your Azure OpenAI credentials:
   ```env
   AZURE_OPENAI_API_KEY=your-api-key-here
   AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
   AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o
   AZURE_OPENAI_API_VERSION=2024-02-15-preview
   ```

## Usage

### Basic Usage

```bash
python respector_llm_enhancer.py respector-generated/restcountries.json
```

This creates `respector-enhanced-generated/restcountries.json` with generated documentation.

### Custom Output Path

```bash
python respector_llm_enhancer.py respector-generated/restcountries.json -o output/enhanced_spec.json
```

### Verbose Mode

```bash
python respector_llm_enhancer.py respector-generated/restcountries.json -v
```

Shows progress for each endpoint being processed.

### Full Options

```bash
python respector_llm_enhancer.py --help
```

## Viewing the Results in Swagger UI

### Start the Documentation Viewers

Use Docker Compose to run two side-by-side Swagger UI instances:

```bash
# Start both viewers
./view-docs.sh

# Or manually:
docker-compose up -d
```

This will start:
- **BEFORE viewer** (Raw Respector output): http://localhost:9081
- **AFTER viewer** (Enhanced documentation): http://localhost:9082

### Navigate the Documentation

1. Open both URLs in your browser (they should open automatically)
2. Arrange windows side-by-side for easy comparison
3. Navigate to the same endpoint in both viewers
4. Compare the differences:
   - **Left (BEFORE)**: Technical structure only, empty descriptions
   - **Right (AFTER)**: Full documentation with summaries and descriptions

### Stop the Viewers

When you're done viewing:

```bash
# Stop and remove containers
docker-compose down

# Or if you used ./view-docs.sh with Ctrl+C, clean up with:
docker-compose down
```

### Alternative Viewing Methods

**Option 1: Online Swagger Editor** (no installation needed)
1. Go to [editor.swagger.io](https://editor.swagger.io)
2. File → Import file → Select your JSON spec
3. View documentation in the right panel

**Option 2: VS Code Extension**
1. Install "Swagger Viewer" extension
2. Open any `.json` spec file
3. Press `Shift+Alt+P` → "Preview Swagger"

**Option 3: NPX** (one-time, no installation)
```bash
npx --yes swagger-ui-watcher respector-generated/order-api.json
```

## Example

### Input (Respector Output)
```json
{
  "/v1/alpha/{alphacode}": {
    "get": {
      "operationId": "em17",
      "parameters": [
        {
          "name": "alphacode",
          "in": "path",
          "required": true,
          "schema": {
            "type": "string",
            "maxLength": 3,
            "minLength": 2
          }
        }
      ],
      "responses": {
        "200": { "description": "OK" },
        "400": { "description": "BAD_REQUEST" },
        "404": { "description": "NOT_FOUND" }
      }
    }
  }
}
```

### Output (Enhanced)
```json
{
  "/v1/alpha/{alphacode}": {
    "get": {
      "operationId": "em17",
      "summary": "Get Country by Alpha Code",
      "description": "Retrieves detailed country information using ISO alpha-2 or alpha-3 country codes. The alphacode parameter must be between 2-3 characters. Returns 400 for invalid codes and 404 if the country is not found.",
      "x-enhanced-by": "Respector-LLM",
      "parameters": [...],
      "responses": {...}
    }
  }
}
```

## Architecture

```
┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│  Respector      │────▶│  JSON Parser │────▶│ Context         │
│  Output (JSON)  │     │              │     │ Extractor       │
└─────────────────┘     └──────────────┘     └────────┬────────┘
                                                      │
                                                      ▼
┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│  Enhanced Spec  │◀────│  Semantic    │◀────│ Prompt Builder  │
│  (JSON)         │     │  Injector    │     │                 │
└─────────────────┘     └──────────────┘     └────────┬────────┘
                                                      │
                                                      ▼
                                             ┌─────────────────┐
                                             │  Azure OpenAI   │
                                             │  GPT-4o         │
                                             └─────────────────┘
```

## Constraints & Considerations

- **Cost**: Processing APIs with many endpoints incurs Azure OpenAI token costs
- **Latency**: Text generation adds ~1-2 seconds per endpoint to the pipeline
- **AI Hallucinations**: If method names are ambiguous (e.g., `doProcess()`), the AI may generate generic or incorrect descriptions

## Future Improvements

- [ ] **Local Inference**: Support for local models (Llama 3 via Ollama) to eliminate costs and improve data privacy
- [ ] **Diff Checker**: UI module to highlight differences between old and new documentation  
- [ ] **CI/CD Integration**: GitHub Action that runs automatically on each Pull Request

## License

MIT License - See [LICENSE](LICENSE) for details.
